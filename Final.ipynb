{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbdb0ed-6fc4-4f16-a7bd-a3e6b6eaa1ac",
   "metadata": {},
   "source": [
    "# Scraping Top Repositories for Topics on GitHub \n",
    "\n",
    "TODO :\n",
    " 1> Intro About The Web Scrapping\n",
    " 2> Intro about GitHub and Problem Statement\n",
    " 3> Tools Used(Python, requests,BeautifulSoup,Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddcbb9f-ed28-4003-a17d-3eb8c6c7e9cb",
   "metadata": {},
   "source": [
    "# Here are the steps which we followed:\n",
    "\n",
    "1> We're going to Scrape \"https://github.com/topics\"\n",
    "2> We'll get list of topics For each topic we will get topic title , topic url ,topic discription\n",
    "3> for each individual topic we will get its repo name , user name , stars and repo URL\n",
    "4> For each topic we will create a CSV file \n",
    "\n",
    "# Scape the list of topics from GitHub\n",
    "- use requests to download the page\n",
    "- use BS4 to parse and extract information\n",
    "- convert to pandas dataframe\n",
    "\n",
    "# Lets Write The Function To Download the page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7860b52-7209-4aa7-81c5-9ae38d272e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def get_topics_page():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3d135f-80b9-4fdc-aa65-ccf1b485f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topics_page()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99485301-3d17-4cdb-a230-cac0181b9272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"px-2 py-4 color-bg-accent-emphasis color-fg-on-emphasis show-on-focus js-skip-to-content\" data-skip-target-assigned=\"false\" href=\"#start-of-content\">Skip to content</a>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.find('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d31a5-32d0-465d-981a-8b28b5c7f29a",
   "metadata": {},
   "source": [
    "# Lets create some helper function to parse information from the page\n",
    "\n",
    "To get topic titles, we can pick 'p' tags with the 'class = f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "\n",
    "![](https://imgur.com/a/fpTKCcr.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f18590c3-2070-4f93-9ea2-4cf45092662e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class =\"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "    topic_title_tags = doc.find_all('p',{'class': selection_class})\n",
    "    topic_titles =[]\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e0b7da-9afe-427a-adbf-272497220f5b",
   "metadata": {},
   "source": [
    "- get_topic_titles Can be used to get the list of titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bff143f-e417-4fb1-a7a3-78a8623fa423",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_topic_titles(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8aa6d351-cd46-4a84-a56d-0c978a9e5e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ac5fa0c-a852-49ad-8317-a70058c302b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4842c-85a9-4960-a4aa-31f924b04342",
   "metadata": {},
   "source": [
    "Similarly we have defined function for description and URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f3c0a-af23-4347-82f2-5759981af36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_desc(doc):\n",
    "    \n",
    "    desc_selector = 'f5 color-fg-muted mb-0 mt-1'\n",
    "    topic_desc_tags = doc.find_all('p',{'class': desc_selector})\n",
    "    topic_desc =[]\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_desc.append(tag.text.strip())\n",
    "    return topic_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15c0f61f-c00d-44a2-9937-5db69653145b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_url(doc):\n",
    "    topic_link_tags = doc.find_all('a' , {'class': 'no-underline flex-1 d-flex flex-column'})\n",
    "    topic_url =[]\n",
    "    base_url ='https://github.com'\n",
    "    for tag in topic_link_tags:\n",
    "        topic_url.append(base_url + tag['href'].strip())\n",
    "    return topic_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2e1c90-7f54-42b9-9f70-51e48df43efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56964b4a-7b4b-4f51-a817-01fcff835825",
   "metadata": {},
   "source": [
    "Lets pull this all together into the single function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e507e5ad-961f-45b9-9cad-4fdf329120d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    topics_dict = {\n",
    "        'title' : get_topic_titles(doc),\n",
    "        'description' : get_topic_desc(doc),\n",
    "        'url' :get_topic_url(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c9d8fe65-e277-48d0-b3f5-b855cc241c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://github.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ab07f-2be9-4439-8bef-3ee9fa75d0c5",
   "metadata": {},
   "source": [
    "# Get the top 25 repositories from a topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ec0ad3ea-100b-4a66-8bef-6039b53c3aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    return BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f0409a2d-c219-4fb5-ada7-b43833e938d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page('https://github.com/topics/3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4878b1f-d142-4eac-bffe-a76262187398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tag, star_tag):\n",
    "    # Returns required information about a repository\n",
    "    a_tags = h3_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = base_url + a_tags[1]['href']\n",
    "    stars = star_tag.text.strip()  # Remove any extra whitespace\n",
    "    return username, repo_name, repo_url, stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1869648a-a7fe-428a-b5f8-cdea881c3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_topic_repos(topic_doc):\n",
    "    repo_tags = topic_doc.find_all('h3', {'class': 'f3 color-fg-muted text-normal lh-condensed'})\n",
    "    star_tags = topic_doc.find_all('span', {'class': 'Counter js-social-count'})\n",
    "\n",
    "    if len(repo_tags) < 20 or len(star_tags) < 20:\n",
    "        raise Exception(\"Not enough repositories found on the page.\")\n",
    "\n",
    "    topic_repos_dict = {\n",
    "        'username': [],\n",
    "        'repo_name': [],\n",
    "        'repo_url': [],\n",
    "        'stars': []\n",
    "    }\n",
    "    for i in range(20):  # Fetch only the first 20 repositories\n",
    "        info = get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repos_dict['username'].append(info[0])\n",
    "        topic_repos_dict['repo_name'].append(info[1])\n",
    "        topic_repos_dict['repo_url'].append(info[2])\n",
    "        topic_repos_dict['stars'].append(info[3])\n",
    "\n",
    "    return pd.DataFrame(topic_repos_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8699dcef-a2d7-426d-bdb5-92e7f7c80109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url ,path):\n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(path + '.csv', index =None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9abdb-b3f0-4c50-b748-7d9e9590e7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d433f185-921e-40bb-9fe3-9aa113f429d4",
   "metadata": {},
   "source": [
    "# Putting It Together\n",
    "- We have the function to get list of topics\n",
    "- We have function to create a CSV File for scapped from topic page\n",
    "- lets create a function to put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fe1177f7-064b-437e-aa77-fd5c89af7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def scrape_topics_repos():\n",
    "    print('Scrapping list of topics ')\n",
    "    topics_df = scrape_topics()\n",
    "    os.makedirs('Data', exist_ok=True)\n",
    "    for index, row in topics_df.iterrows():\n",
    "        print('scrapping top repositories for\"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'],'Data/{}.csv'.format(row['title']))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6fd10-4951-47e4-bfa4-47a52784e6d9",
   "metadata": {},
   "source": [
    "Lets run it to scrape the top repos for all the topics on the first page of \n",
    "'https://github.com/topics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9934759b-9c58-4a60-bc3d-b44fa41167be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb778a-012d-4bd3-acf4-b4b81c8c73e2",
   "metadata": {},
   "source": [
    "# Thank You and here I conclude my short Web Scrapping Live Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fda0bd-9641-4cc1-86d0-a6392b6e339f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
